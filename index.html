<html>
<head>
<title>Joint Learning of Gaze and Actions in First Person Vision</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 48px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}


h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 0px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 30px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 960px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

td img {
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1><span style="color: #DE3737"> Geogia Institute of Technology</span></h1>
</div>
</div>
<div class="container">
  
<h2>Joint Learning of Gaze and Actions in First Person Vision</h2>

<h4>Yin Li, Miao Liu, James Rehg</h4>

<h3>Abstract:</h3>

<p>
We address the task of jointly determining what a person is doing and where
they are looking based on the analysis of video captured by a headworn camera. 
We propose a novel deep model for joint gaze estimation and action recognition 
in First Person Vision. Our method describes the participant's gaze as a 
probabilistic variable and models its distribution using stochastic units 
in a deep network. We sample from these stochastic units to generate an attention map. 
This attention map guides the aggregation of visual features in action recognition, 
thereby providing coupling between gaze and action. We evaluate our method on the s
tandard EGTEA dataset and demonstrate performance that exceeds the state-of-the-art 
by a significant margin of 3.5%.
</p>

<div style="float: right; padding: 50px">
<img src="overview.png" width="900" height="400">
<p style="font-size: 14px">Overview of our method</p>
</div>

<p>
You can download our paper <a href="https://www.dropbox.com/s/8rjirv5s3q232so/eccv18-gaze-action.pdf?dl=0">here</a>.	
</p>

<p>
You are welcomed to use our <a href="http://www.cbi.gatech.edu/fpv/">dataset</a>.
</p>

<p>
If you find our paper helpful, please consider cite our paper.
</p>

<h3>Contact:</h3>	
<p>
Yin Li: yli440@gatech.edu
</p>
<p>
Miao Liu: mliu328@gatech.edu
</p>


</body>
<html>
